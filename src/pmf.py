# -*- coding: utf-8 -*-
"""PMF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Up_PkB9NAoDJKY7g0NoIIG3Ge9eZQ1Dh

PART 1: Probabilistic matrix Factorisation to fill the empty values
"""

import numpy as np
import pandas as pd
import seaborn as sb
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error

df = pd.read_excel("U.xlsx")
df.head()

df_subjects = pd.read_excel("V (1).xlsx")
df_subjects.head()

df_subjects = pd.read_excel("V (1).xlsx")
df_join = pd.merge(df_subjects, df, how='inner', on='Paper_ID')
df_join.head()

def get_marks_matrix(df, train_size=0.75):
    student_to_row = {}
    subject_to_column = {}
    df_values = df.values
    n_dims = 10
    parameters = {}

    uniq_students = np.unique(df_values[:, 17])
    uniq_subjects = np.unique(df_values[:, 0])

    for i, Student_ID in enumerate(uniq_students):
        student_to_row[Student_ID] = i

    for j, Paper_ID in enumerate(uniq_subjects):
        subject_to_column[Paper_ID] = j

    n_students = len(uniq_students)
    n_subjects = len(uniq_subjects)

    R = np.zeros((n_students, n_subjects))

    df_copy = df.copy()
    train_set = df_copy.sample(frac=train_size, random_state=0)
    test_set = df_copy.drop(train_set.index)

    for index, row in train_set.iterrows():
        i = student_to_row[row.Student_ID]
        j = subject_to_column[row.Paper_ID]
        R[i, j] = row.Marks

    return R, train_set, test_set, n_dims, n_students, n_subjects, student_to_row, subject_to_column

R, train_set, test_set, n_dims, n_students, n_subjects, student_to_row, subject_to_column = get_marks_matrix(df_join, 0.75)
parameters = {}

def initialize_parameters(lambda_U, lambda_V):
    U = np.zeros((n_dims, n_students), dtype=np.float64)
    V = np.random.normal(0.0, 1.0 / lambda_V, (n_dims, n_subjects))
    parameters['U'] = U
    parameters['V'] = V
    parameters['lambda_U'] = lambda_U
    parameters['lambda_V'] = lambda_V

"""
Let's now implement the function that updates U and V. The elements of both matrices can be updated using the following expressions:

$$
\large
U_i=\left[\left(V_jV_j^T\right)_{j\in\Omega_{U_i}}+\lambda_UI\right]^{-1}\left(R_{ij}V_j^T\right)_{j\in\Omega_{U_i}}
$$$$
\large
V_j=\left[\left(U_iU_i^T\right)_{i\in\Omega_{V_j}}+\lambda_VI\right]^{-1}\left(R_{ij}U_i^T\right)_{i\in\Omega_{V_j}}
$$"""

def update_parameters():
    U = parameters['U']
    V = parameters['V']
    lambda_U = parameters['lambda_U']
    lambda_V = parameters['lambda_V']

    for i in range(n_students):
        V_j = V[:, R[i, :] > 0]
        U[:, i] = np.dot(np.linalg.inv(np.dot(V_j, V_j.T) + lambda_U * np.identity(n_dims)), np.dot(R[i, R[i, :] > 0], V_j.T))

    for j in range(n_subjects):
        U_i = U[:, R[:, j] > 0]
        V[:, j] = np.dot(np.linalg.inv(np.dot(U_i, U_i.T) + lambda_V * np.identity(n_dims)), np.dot(R[R[:, j] > 0, j], U_i.T))

    parameters['U'] = U
    parameters['V'] = V

def log_a_posteriori():
    lambda_U = parameters['lambda_U']
    lambda_V = parameters['lambda_V']
    U = parameters['U']
    V = parameters['V']

    UV = np.dot(U.T, V)
    R_UV = (R[R > 0] - UV[R > 0])

    return -0.5 * (np.sum(np.dot(R_UV, R_UV.T)) + lambda_U * np.sum(np.dot(U, U.T)) + lambda_V * np.sum(np.dot(V, V.T)))

def predict(Student_ID, Paper_ID):
    U = parameters['U']
    V = parameters['V']

    r_ij = U[:,student_to_row[Student_ID]].T.reshape(1, -1) @ V[:, subject_to_column[Paper_ID]].reshape(-1, 1)

    max_mark = parameters['max_mark']
    min_mark = parameters['min_mark']

    return 0 if max_mark == min_mark else ((r_ij[0][0] - min_mark) / (max_mark - min_mark)) * 100

def evaluate(dataset):
    ground_truths = []
    predictions = []

    for index, row in dataset.iterrows():
        ground_truths.append(row.loc['Marks'])
        predictions.append(predict(row.loc['Student_ID'], row.loc['Paper_ID']))

    return mean_squared_error(ground_truths, predictions, squared=False)

def update_max_min_marks():
    U = parameters['U']
    V = parameters['V']

    R = U.T @ V
    min_mark = np.min(R)
    max_mark = np.max(R)

    parameters['min_mark'] = min_mark
    parameters['max_mark'] = max_mark

def train(n_epochs):
    initialize_parameters(0.3, 0.3)
    log_aps = []
    rmse_train = []
    rmse_test = []

    update_max_min_marks()
    rmse_train.append(evaluate(train_set))
    rmse_test.append(evaluate(test_set))

    for k in range(n_epochs):
        update_parameters()
        log_ap = log_a_posteriori()
        log_aps.append(log_ap)

        if (k + 1) % 10 == 0:
            update_max_min_marks()

            rmse_train.append(evaluate(train_set))
            rmse_test.append(evaluate(test_set))
            print('Log p a-posteriori at iteration', k + 1, ':', log_ap)

    update_max_min_marks()

    return log_aps, rmse_train, rmse_test

log_ps, rmse_train, rmse_test = train(300)

print('RMSE of training set:', evaluate(train_set))
print('RMSE of testing set:', evaluate(test_set))

_, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
plt.title('Training results')
ax1.plot(np.arange(len(log_ps)), log_ps, label='MAP')
ax1.legend()

ax2.plot(np.arange(len(rmse_train)), rmse_train, label='RMSE train')
ax2.plot(np.arange(len(rmse_test)), rmse_test, label='RMSE test')
ax2.legend()

plt.show()

sb.heatmap(R,cmap="YlGnBu")
plt.show()

sample=df_join.loc[df_join['Student_ID'] == 1171]
sample

Student_ID = 1143
df_join[df_join['Student_ID'] == Student_ID].sort_values(by=['Marks'], ascending=False).head(10)

df_join[df_join['Student_ID'] == Student_ID].sort_values(by=['Marks']).head(10)

predictions = np.zeros((n_subjects, 1))
subject_to_column_items = np.array(list(subject_to_column.items()))
df_result = pd.DataFrame(columns=['StudentId', 'subject_id', 'subject', 'Prediction'])

for i, subject in enumerate(subject_to_column_items):
    predictions[i] = predict(Student_ID, subject[0])

indices = np.argsort(-predictions, axis=0)

for j in range(10):
    Paper_ID = int(subject_to_column_items[np.where(subject_to_column_items[:, 1] == indices[j])][0][0])
    df_row = pd.DataFrame({
        'StudentId': Student_ID,
        'subject_id': Paper_ID,
        'subject': df_subjects[df_subjects['subject_id'] == Paper_ID].iloc[0]['Paper_Name'],
        #'Genres': df_subjects[df_movies['subject_id'] == Paper_ID].iloc[0]['genres'],
        'Prediction': predictions[indices[j]][0][0]
    }, index=[j])
    df_result = df_result.append(df_row, sort=False)

df_result

"""PART 2:Base predictors using voting ensembled predictions"""

import sklearn as sk
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
dataframe = pd.read_excel("Semester-1.xlsx")
array = dataframe.values
X = array[:,0:6]
Y = array[:,6]
kfold = sk.model_selection.KFold(n_splits=10, random_state=10)
# create the sub models
estimators = []
model1 = LogisticRegression()
estimators.append(('logistic', model1))
model2 = DecisionTreeClassifier()
estimators.append(('cart', model2))
model3 = SVC()
estimators.append(('svm', model3))
# create the ensemble model
ensemble = VotingClassifier(estimators)
results = sk.model_selection.cross_val_score(ensemble, X, Y, cv=kfold)
print(results.mean())